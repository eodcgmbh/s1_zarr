{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516d0fd0",
   "metadata": {},
   "source": [
    "### Creating an ArgoWorkflow with hera for INCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b4abf9",
   "metadata": {},
   "source": [
    "This notebook shows how to create an ArgoWorkflow to write data to a zarr store for INCA data. The created workflow will write new data to the store once a day. To learn more about Hera visit https://hera.readthedocs.io/en/v4/index.html and https://hera.readthedocs.io/en/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0871cbe",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98851abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from hera.workflows import models, CronWorkflow, script, Artifact, Parameter, DAG, Steps, Step, NoneArchiveStrategy, Workflow\n",
    "from hera.shared import global_config\n",
    "\n",
    "load_dotenv(\"/home/otto/s1_zarr/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9e2766",
   "metadata": {},
   "source": [
    "### Global Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61377ffc",
   "metadata": {},
   "source": [
    "First the host, token, namespace and used image are set globally, as they stay don't change for different steps in this notebook. You can look at on how to create an image. It should have all the depencies you need to execute your scripts in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7344659",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_config.host = \"https://services.eodc.eu/workflows/\"\n",
    "global_config.namespace = \"inca\"\n",
    "global_config.token = os.getenv(\"argo_token_prod\")\n",
    "global_config.image = \"ghcr.io/oscipal/image_zarr:latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716ba6e6",
   "metadata": {},
   "source": [
    "To get access to the EODC NFS, you need to define how the volume is is accessed. This is not default for all namespaces and might have to be set up. To have write access you need to define the correct security access. The runAsUser value is the UserID of the folder you want to write to, and the runAsGroup value the GroupID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8596c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfs_volume = [models.Volume(\n",
    "    name=\"eodc-mount\",\n",
    "    persistent_volume_claim={\"claimName\": \"eodc-nfs-claim\"},\n",
    "    )]\n",
    "\n",
    "security_context = {\"runAsUser\": 74268,\n",
    "                    \"runAsGroup\": 71473}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e17c74",
   "metadata": {},
   "source": [
    "### Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1226747d",
   "metadata": {},
   "source": [
    "There are four ways to store data in a Workflow, *empty directories*, *NFS*, *artifacts* and *OpenStack via Cinder CSI*. For this case we will only go into artifact and NFS storage options. Data stored as an Artifact can be passed between pods in the workflow but artifact storage is only temporary meaning the file will be deleted after completion of the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c56d99",
   "metadata": {},
   "source": [
    "You can write python scripts you want to use in your workflow under the `@script` decorator. You have to define if you use any storage options, and any inputs and outputs you want to use in the decorator. But more in that later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcab5c1",
   "metadata": {},
   "source": [
    "As a first step we want to extend the time dimension in the zarr store to be able to append new data. We need to access the NFS to have access to the zarr store, so we have to define the volume mount in the `@script` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9350f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@script(volume_mounts=[models.VolumeMount(name=\"eodc-mount\", mount_path=\"/eodc\")])\n",
    "\n",
    "def extend_time_dimension(store_path: str = \"/eodc/private/openeo_platform/zarr_nacho/INCA_test.zarr\"):\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    import zarr\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    now_np = np.datetime64(now).astype('datetime64[h]')\n",
    "    origin = np.datetime64(\"2011-03-15T00:00:00\").astype(\"datetime64[h]\")\n",
    "\n",
    "    new_shape = int((now_np-origin).astype(int))\n",
    "    new_extent = np.arange(0,new_shape,1)\n",
    "\n",
    "    store = zarr.storage.LocalStore(store_path)\n",
    "    group = zarr.group(store=store)\n",
    "\n",
    "    array_names=set(group.array_keys())\n",
    "    coords = {\"time\", \"x\", \"y\"}\n",
    "    data_arrays = array_names-coords\n",
    "\n",
    "    group[\"time\"].resize(new_shape)\n",
    "    for array in data_arrays:\n",
    "        group_shape  = group[array].shape\n",
    "        group[array].resize((new_shape, group_shape[1], group_shape[2]))\n",
    "\n",
    "    zarr.consolidate_metadata(store)\n",
    "    store = zarr.storage.LocalStore(store_path)\n",
    "    group = zarr.group(store=store)\n",
    "\n",
    "    group[\"time\"][:]=new_extent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbff2dc",
   "metadata": {},
   "source": [
    "Next we want to download data. We don't need to access the NFS for this step, but we do need to create an Artifact to pass the downloaded file to the next step. Again, this has to be defined in the `@script` decorator, as it is an output of the step it is defined as `output`. The name of the Artifact is like a variable name and can be freely chosen, it just needs to be correctly referenced when creating the workflow, more on that later. The path to the file has to be under `/tmp`, multiple files can also be downloaded to a folder in this path. Use *NoneArchiveStrategy()* to not compress it to .tgz format as this can cause issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2768af",
   "metadata": {},
   "outputs": [],
   "source": [
    "@script(outputs=Artifact(name=\"inca-file\", path=\"/tmp/INCA.nc\", archive=NoneArchiveStrategy()))\n",
    "\n",
    "def inca_download(param: str):\n",
    "    from urllib.request import urlretrieve\n",
    "    import datetime\n",
    "\n",
    "    ym = (datetime.date.today()-datetime.timedelta(days=8)).strftime(\"%Y%m\")\n",
    "    print(ym)\n",
    "    url = f\"https://public.hub.geosphere.at/datahub/resources/inca-v1-1h-1km/filelisting/{param}/INCAL_HOURLY_{param}_{ym}.nc\"\n",
    "    urlretrieve(url, f\"/tmp/INCA.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f033d4",
   "metadata": {},
   "source": [
    "As a last step we want to write the data to the zarr store on the NFS. So we need to mount the NFS like in the first step, and, as we also need the artifact from the download step, we need to pass this under the `inputs` parameter. The name of the artifact does not have to be the same as in the download step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ea397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@script(inputs=Artifact(name=\"inca-file\", path=\"/tmp/INCA.nc\"),\n",
    "        volume_mounts=[models.VolumeMount(name=\"eodc-mount\", mount_path=\"/eodc\")])\n",
    "\n",
    "def inca_write(param: str, store_path: str=\"/eodc/private/openeo_platform/zarr_nacho/INCA_test.zarr\"):\n",
    "    import xarray as xr\n",
    "    import numpy as np\n",
    "    import zarr\n",
    "    import pandas as pd\n",
    "\n",
    "    artifact_path = f\"/tmp/INCA.nc\"\n",
    "\n",
    "    def get_idx(array1, array2):\n",
    "        min_idx = np.where(array1 == array2[0])[0][0]\n",
    "        max_idx = np.where(array1 == array2[-1])[0][0] + 1\n",
    "        return min_idx, max_idx\n",
    "\n",
    "    store = zarr.storage.LocalStore(store_path)\n",
    "    group = zarr.group(store=store)\n",
    "\n",
    "    dtype = group[param].dtype\n",
    "    fill_value = group[param].attrs.get('_FillValue')\n",
    "    freq = group.attrs.get('freq')\n",
    "\n",
    "    x_extent = group[\"x\"][:]\n",
    "    y_extent = group[\"y\"][:]\n",
    "    origin = xr.open_zarr(store_path).time[0].values\n",
    "\n",
    "    data = xr.open_dataset(artifact_path, mask_and_scale=False).load()\n",
    "\n",
    "    x_min, x_max = get_idx(x_extent, data[\"x\"].values)\n",
    "    y_min, y_max = get_idx(y_extent, data[\"y\"].values)\n",
    "\n",
    "    time_min, time_max = data.time.values[0].astype(\"datetime64[h]\"), data.time.values[-1].astype(\"datetime64[h]\") + 1\n",
    "    time_delta_min, time_delta_max = (time_min - origin).astype(\"int64\"), (time_max - origin).astype(\"int64\")\n",
    "\n",
    "    full_range = pd.date_range(time_min, time_max, freq=freq).values.astype(\"datetime64[ns]\")\n",
    "\n",
    "    for value in data.time.values:\n",
    "        if value in set(full_range):\n",
    "            continue\n",
    "        else:\n",
    "            empty_array = np.full((full_range.shape[0], data[\"x\"].values.shape[0], data[\"y\"].values.shape[0]),\n",
    "                                fill_value=fill_value, dtype=dtype)\n",
    "\n",
    "            template = xr.Dataset({f\"{param}\": ((\"time\", \"x\", \"y\"), empty_array)},\n",
    "                                  coords={\n",
    "                                    \"time\": full_range,\n",
    "                                    \"x\": data[\"x\"].values,\n",
    "                                    \"y\": data[\"y\"].values\n",
    "                                  }\n",
    "                                  )\n",
    "\n",
    "            data = data.combine_first(template)\n",
    "            break\n",
    "\n",
    "    group[param][time_delta_min:time_delta_max, y_min:y_max, x_min:x_max] = data[param].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1194bb8",
   "metadata": {},
   "source": [
    "### Creating the workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65661acf",
   "metadata": {},
   "source": [
    "As the scripts only act as template Workflows we still need to define a workflow to let Argo know how to execute the scripts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2282d41c",
   "metadata": {},
   "source": [
    "As we have 8 different parameter for our INCA data we want the processing for them to be done in parallel. The syntax for Hera can be quite confusing, but the code below creates a CronWorkflow to achieve exactly what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2764af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "inca_parameters = [\"RR\", \"T2M\", \"TD2M\", \"P0\", \"UU\", \"VV\", \"RH2M\", \"GL\"]\n",
    "\n",
    "# At first a CronWorkflow is created with the necessary inputs.\n",
    "with Workflow(\n",
    "    generate_name=\"inca-zarr-\",\n",
    "    #schedule=\"41 11 * * *\",\n",
    "    volumes = nfs_volume,\n",
    "    security_context=security_context,\n",
    "    entrypoint=\"workflow\"\n",
    ") as w:\n",
    "    \n",
    "    # Secondly, a DAG is defined which shall be executed for each parameter. The inputs are defined in the Steps below. So this DAG acts like a function being defined and executed in a different step.\n",
    "    with DAG(name=\"pipeline\", inputs=[Parameter(name=\"args\")]) as pipeline:\n",
    "        \n",
    "        # The arguments for the scripts are passed as a dictionary, wherease the arguments for the 'param' parameter are taken from the input of the DAG\n",
    "        download = inca_download(arguments={\"param\":\"{{inputs.parameters.args}}\"},)\n",
    "\n",
    "        # The Artifact written with download also has to be given to the function.\n",
    "        process = inca_write(arguments=[{\"param\": \"{{inputs.parameters.args}}\"}, \n",
    "                                        download.get_artifact(\"inca-file\").with_name(\"inca-file\")],)\n",
    "\n",
    "        # Here the sequence of the steps is defined\n",
    "        download >> process\n",
    "\n",
    "    # As we defined \"workflow\" as the entrypoint in the CronWorkflow this part gets executed first, in contrast to DAGs Steps will be executed in the order they are in\n",
    "    with Steps(name=\"workflow\"):\n",
    "        # First the time dimension is extended in the zarr store\n",
    "        extend_time_dimension()\n",
    "\n",
    "        # Now the DAG is executed, it is used as a template, passing the inca_parameters as with_param and using \"{{item}}\" in arguments the DAG will be executed parallel for each parameter.\n",
    "        Step(name=\"parallel-pipelines\", template=pipeline, with_param=inca_parameters, arguments={\"args\":\"{{item}}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f9eaed",
   "metadata": {},
   "source": [
    "The Workflow can be written to a yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a28670",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"INCA_workflow.yaml\", \"w\") as f:\n",
    "    f.write(w.to_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61f9933",
   "metadata": {},
   "source": [
    "Or passed directly to ArgoWorkflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb7181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c31e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zarr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
