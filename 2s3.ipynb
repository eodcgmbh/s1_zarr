{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647eaa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import s3fs\n",
    "from dotenv import load_dotenv\n",
    "import botocore\n",
    "import boto3\n",
    "import os\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd8dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers=8 \n",
    "max_pool_conn = 3 * n_workers\n",
    "if max_pool_conn < 60: max_pool_conn = 60\n",
    "client_config = botocore.config.Config(\n",
    "    max_pool_connections=max_pool_conn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2fe24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"s3.env\") \n",
    "\n",
    "endpoint_url = 'https://objectstore.eodc.eu:2222'\n",
    "aws_access_key_id = os.getenv(\"key\")\n",
    "aws_secret_access_key = os.getenv(\"secret\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3363cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=endpoint_url,\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    config=client_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f14d911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_storage_class(fsize):\n",
    "\n",
    "    small = 16384       # 16KB = 16*1024\n",
    "    medium = 16777216   # 16MB = 16*1024**2\n",
    "\n",
    "    if fsize < small:\n",
    "        storage_class = \"EXPRESS_ONEZONE\"\n",
    "    elif fsize < medium:\n",
    "        storage_class = \"STANDARD_3X\"\n",
    "    else:\n",
    "        storage_class = \"STANDARD\"\n",
    "\n",
    "    return storage_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e186455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_one_file(client, bucket_name, fname, object_name, storage_class):\n",
    "    if storage_class is None:\n",
    "        client.upload_file(fname, bucket_name, object_name) \n",
    "    else:\n",
    "        client.upload_file(fname, bucket_name, object_name,\n",
    "                           ExtraArgs={'StorageClass': storage_class}\n",
    "                           ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56513db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_files = set()\n",
    "\n",
    "paginator = client.get_paginator('list_objects_v2')\n",
    "for page in paginator.paginate(Bucket=output_s3_bucket, Prefix=\"s1sig0.zarr\"):\n",
    "    for obj in page.get('Contents', []):\n",
    "        existing_files.add(obj['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bdeaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a filtered list: only files whose S3 key is NOT already in the bucket\n",
    "new_ftu = [file_tuple for file_tuple in files_to_upload if file_tuple[1] not in existing_files]\n",
    "\n",
    "print(f\"Total files to upload: {len(new_ftu)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cf1c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_upload= []\n",
    "output_zarr=\"s1sig0.zarr\"\n",
    "root_uri=\"\"\n",
    "remote_dir=\"\"\n",
    "bname = 's1sig0.zarr'\n",
    "remote_zarr_uri = os.path.join(root_uri, remote_dir, bname)\n",
    "\n",
    "for root, dirs, files in os.walk(output_zarr):\n",
    "    for file in files:\n",
    "        fpath = os.path.join(root, file)\n",
    "        relpath = os.path.relpath(fpath, output_zarr)\n",
    "        remote_output_path = f\"{remote_zarr_uri}/{relpath}\"\n",
    "        fsize = os.path.getsize(fpath)\n",
    "        storage_class = get_storage_class(fsize)\n",
    "        files_to_upload.append((fpath, remote_output_path, \n",
    "                                \"STANDARD\", fsize))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9763cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_s3_bucket = 'S1Sig0'\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "    futures = [executor.submit(upload_one_file, client, output_s3_bucket, \n",
    "                                fname, obj_name, storage_class) \n",
    "                for fname, obj_name, storage_class, _ in new_ftu]\n",
    "    concurrent.futures.wait(futures, timeout=None, \n",
    "                            return_when=concurrent.futures.ALL_COMPLETED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ed3f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_path = f\"s3://S1Sig0/s1sig0.zarr\"\n",
    "ds = xr.open_zarr(store=store_path, group='AT', consolidated=True, chunks={}, storage_options={\n",
    "    'key': aws_access_key_id,\n",
    "    'secret': aws_secret_access_key,\n",
    "    'client_kwargs': {'endpoint_url': 'https://objectstore.eodc.eu:2222'}\n",
    "}).sel(time=slice(\"2024-01-01T00:00:00.000000000\",\"2024-03-01T00:00:00.000000000\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e20b373",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
